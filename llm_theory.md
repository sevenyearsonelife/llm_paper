# paper
[[2205.14135] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (arxiv.org)](https://arxiv.org/abs/2205.14135)

[[2307.08691] FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning (arxiv.org)](https://arxiv.org/abs/2307.08691)